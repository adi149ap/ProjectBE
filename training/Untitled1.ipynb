{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1006 04:20:34.894607  7792 deprecation_wrapper.py:119] From C:\\Users\\LaukikR\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W1006 04:20:35.286647  7792 deprecation_wrapper.py:119] From C:\\Users\\LaukikR\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W1006 04:20:35.471311  7792 deprecation_wrapper.py:119] From C:\\Users\\LaukikR\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W1006 04:20:35.746452  7792 deprecation_wrapper.py:119] From C:\\Users\\LaukikR\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "W1006 04:20:37.831704  7792 deprecation_wrapper.py:119] From C:\\Users\\LaukikR\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W1006 04:20:37.831704  7792 deprecation_wrapper.py:119] From C:\\Users\\LaukikR\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 InputLayer False\n",
      "1 Conv2D False\n",
      "2 Conv2D False\n",
      "3 MaxPooling2D False\n",
      "4 Conv2D False\n",
      "5 Conv2D False\n",
      "6 MaxPooling2D False\n",
      "7 Conv2D False\n",
      "8 Conv2D False\n",
      "9 Conv2D False\n",
      "10 MaxPooling2D False\n",
      "11 Conv2D False\n",
      "12 Conv2D False\n",
      "13 Conv2D False\n",
      "14 MaxPooling2D False\n",
      "15 Conv2D False\n",
      "16 Conv2D False\n",
      "17 Conv2D False\n",
      "18 MaxPooling2D False\n"
     ]
    }
   ],
   "source": [
    "from keras.applications import VGG16\n",
    "\n",
    "# Setting the input size now to 64 x 64 pixel \n",
    "img_rows = 64\n",
    "img_cols = 64 \n",
    "\n",
    "# Re-loads the VGG16 model without the top or FC layers\n",
    "vgg16 = VGG16(weights = 'imagenet', \n",
    "                 include_top = False, \n",
    "                 input_shape = (img_rows, img_cols, 3))\n",
    "\n",
    "# Here we freeze the last 4 layers \n",
    "# Layers are set to trainable as True by default\n",
    "for layer in vgg16.layers:\n",
    "    layer.trainable = False\n",
    "    \n",
    "# Let's print our layers \n",
    "for (i,layer) in enumerate(vgg16.layers):\n",
    "    print(str(i) + \" \"+ layer.__class__.__name__, layer.trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 70295 images belonging to 38 classes.\n",
      "Found 17572 images belonging to 38 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1006 04:22:00.779576  7792 deprecation.py:506] From C:\\Users\\LaukikR\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 64, 64, 3)         0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 64, 64, 64)        1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 64, 64, 64)        36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 32, 32, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 32, 32, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 16, 16, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 16, 16, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 16, 16, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 8, 8, 512)         1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 8, 8, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 8, 8, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               524544    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 38)                9766      \n",
      "=================================================================\n",
      "Total params: 15,248,998\n",
      "Trainable params: 534,310\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "def addTopModel(bottom_model, num_classes, D=256):\n",
    "    \"\"\"creates the top or head of the model that will be \n",
    "    placed ontop of the bottom layers\"\"\"\n",
    "    top_model = bottom_model.output\n",
    "    top_model = Flatten(name = \"flatten\")(top_model)\n",
    "    top_model = Dense(D, activation = \"relu\")(top_model)\n",
    "    top_model = Dropout(0.3)(top_model)\n",
    "    top_model = Dense(num_classes, activation = \"softmax\")(top_model)\n",
    "    return top_model\n",
    "\n",
    "from keras.applications import VGG16\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Model\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_data_dir = './train'\n",
    "validation_data_dir = './valid'\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "      rescale=1./255,\n",
    "      rotation_range=20,\n",
    "      width_shift_range=0.2,\n",
    "      height_shift_range=0.2,\n",
    "      horizontal_flip=True,\n",
    "      fill_mode='nearest')\n",
    " \n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    " \n",
    "# Change the batchsize according to your system RAM\n",
    "train_batchsize = 16\n",
    "val_batchsize = 10\n",
    " \n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(img_rows, img_cols),\n",
    "        batch_size=train_batchsize,\n",
    "        class_mode='categorical')\n",
    " \n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        target_size=(img_rows, img_cols),\n",
    "        batch_size=val_batchsize,\n",
    "        class_mode='categorical',\n",
    "        shuffle=False)\n",
    "\n",
    "# Re-loads the VGG16 model without the top or FC layers\n",
    "vgg16 = VGG16(weights = 'imagenet', \n",
    "                 include_top = False, \n",
    "                 input_shape = (img_rows, img_cols, 3))\n",
    "\n",
    "# Freeze layers\n",
    "for layer in vgg16.layers:\n",
    "    layer.trainable = False\n",
    "    \n",
    "# Number of classes in the Flowers-17 dataset\n",
    "num_classes = 38\n",
    "\n",
    "FC_Head = addTopModel(vgg16, num_classes)\n",
    "\n",
    "model = Model(inputs=vgg16.input, outputs=FC_Head)\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1006 04:22:21.747432  7792 deprecation_wrapper.py:119] From C:\\Users\\LaukikR\\Anaconda3\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W1006 04:22:22.159446  7792 deprecation.py:323] From C:\\Users\\LaukikR\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "2196/2196 [==============================] - 2312s 1s/step - loss: 2.3505 - acc: 0.3527 - val_loss: 1.3605 - val_acc: 0.6022\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.36053, saving model to diseases_64.h5\n",
      "Epoch 2/25\n",
      "2196/2196 [==============================] - 2590s 1s/step - loss: 1.5650 - acc: 0.5394 - val_loss: 0.9261 - val_acc: 0.7439\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.36053 to 0.92613, saving model to diseases_64.h5\n",
      "Epoch 3/25\n",
      "2196/2196 [==============================] - 2262s 1s/step - loss: 1.3231 - acc: 0.6028 - val_loss: 1.3159 - val_acc: 0.6089\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.92613\n",
      "Epoch 4/25\n",
      "2196/2196 [==============================] - 2257s 1s/step - loss: 1.1982 - acc: 0.6391 - val_loss: 0.8521 - val_acc: 0.7474\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.92613 to 0.85215, saving model to diseases_64.h5\n",
      "Epoch 5/25\n",
      "2196/2196 [==============================] - 1954s 890ms/step - loss: 1.1195 - acc: 0.6550 - val_loss: 0.5846 - val_acc: 0.8133\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.85215 to 0.58457, saving model to diseases_64.h5\n",
      "Epoch 6/25\n",
      "2196/2196 [==============================] - 1900s 865ms/step - loss: 1.0447 - acc: 0.6777 - val_loss: 1.0300 - val_acc: 0.6867\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.58457\n",
      "Epoch 7/25\n",
      "2196/2196 [==============================] - 1584s 722ms/step - loss: 1.0066 - acc: 0.6874 - val_loss: 0.8232 - val_acc: 0.7450\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.58457\n",
      "Epoch 8/25\n",
      "2196/2196 [==============================] - 1576s 718ms/step - loss: 0.9753 - acc: 0.6988 - val_loss: 0.4809 - val_acc: 0.8401\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.58457 to 0.48091, saving model to diseases_64.h5\n",
      "Epoch 9/25\n",
      "2196/2196 [==============================] - 1558s 709ms/step - loss: 0.9402 - acc: 0.7081 - val_loss: 0.8106 - val_acc: 0.7528\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.48091\n",
      "Epoch 10/25\n",
      "2196/2196 [==============================] - 1571s 715ms/step - loss: 0.9289 - acc: 0.7125 - val_loss: 1.0218 - val_acc: 0.6821\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.48091\n",
      "Epoch 11/25\n",
      "2196/2196 [==============================] - 1695s 772ms/step - loss: 0.9080 - acc: 0.7211 - val_loss: 0.3980 - val_acc: 0.8665\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.48091 to 0.39801, saving model to diseases_64.h5\n",
      "Epoch 12/25\n",
      "2196/2196 [==============================] - 1978s 901ms/step - loss: 0.8855 - acc: 0.7267 - val_loss: 0.5059 - val_acc: 0.8390\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.39801\n",
      "Epoch 13/25\n",
      "2196/2196 [==============================] - 1762s 802ms/step - loss: 0.8744 - acc: 0.7280 - val_loss: 1.0313 - val_acc: 0.6817\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.39801\n",
      "Epoch 14/25\n",
      "2196/2196 [==============================] - 1778s 809ms/step - loss: 0.8626 - acc: 0.7344 - val_loss: 0.5167 - val_acc: 0.8321\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.39801\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 1.9999999494757503e-05.\n",
      "Epoch 15/25\n",
      "2196/2196 [==============================] - 1774s 808ms/step - loss: 0.8316 - acc: 0.7408 - val_loss: 0.4550 - val_acc: 0.8603\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.39801\n",
      "Epoch 16/25\n",
      "2196/2196 [==============================] - 1772s 807ms/step - loss: 0.8237 - acc: 0.7461 - val_loss: 0.9397 - val_acc: 0.7115\n",
      "Restoring model weights from the end of the best epoch\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.39801\n",
      "Epoch 00016: early stopping\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import RMSprop\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "                   \n",
    "checkpoint = ModelCheckpoint(\"diseases_64.h5\",\n",
    "                             monitor=\"val_loss\",\n",
    "                             mode=\"min\",\n",
    "                             save_best_only = True,\n",
    "                             verbose=1)\n",
    "\n",
    "earlystop = EarlyStopping(monitor = 'val_loss', \n",
    "                          min_delta = 0, \n",
    "                          patience = 5,\n",
    "                          verbose = 1,\n",
    "                          restore_best_weights = True)\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor = 'val_loss',\n",
    "                              factor = 0.2,\n",
    "                              patience = 3,\n",
    "                              verbose = 1,\n",
    "                              min_delta = 0.00001)\n",
    "\n",
    "# we put our call backs into a callback list\n",
    "callbacks = [earlystop, checkpoint, reduce_lr]\n",
    "\n",
    "# Note we use a very small learning rate \n",
    "model.compile(loss = 'categorical_crossentropy',\n",
    "              optimizer = RMSprop(lr = 0.0001),\n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "nb_train_samples = 70295\n",
    "nb_validation_samples = 17572\n",
    "epochs = 25\n",
    "batch_size = 32\n",
    "\n",
    "history = model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch = nb_train_samples // batch_size,\n",
    "    epochs = epochs,\n",
    "    callbacks = callbacks,\n",
    "    validation_data = validation_generator,\n",
    "    validation_steps = nb_validation_samples // batch_size\n",
    ")\n",
    "\n",
    "model.save(\"diseases_vgg_64.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
